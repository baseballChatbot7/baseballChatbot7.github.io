I"3<h1 id="kbobert-1">KBOBERT-1</h1>

<h2 id="abstract">Abstract</h2>

<p>야구 도메인에 특화된 BERT로 NER과 MRC를 개발하면 성능이 더 좋을 것 같다고 생각했습니다. 만약 성능이 나쁘다고 하더라도, 대체할 수 있는 pretrain된 BERT들이 존재하기 때문에 부담 없이 진행해볼 수 있었습니다.</p>

<h2 id="data">Data</h2>

<p>우선 KBO와 관련된 뉴스 기사 약 46,000건을 수집했습니다. 이후 야구 관련 위키 데이터와 일반 위키 데이터를 추가하면 pretrain을 하기에는 충분하다고 판단했습니다. KBO 관련 뉴스 데이터 크롤러는 해당 <a href="https://github.com/baseballChatbot7/KBOBERT/blob/main/KBO_NEWS_Crawler.py">repo</a>를 참고하시면 됩니다.</p>

<h2 id="train">Train</h2>

<p>BERT 학습을 위한 코드를 모두 직접 구현한다면 가장 좋겠지만, 시간상의 문제로 어려움이 있었습니다. 그래서 Hugging Face를 사용하여 BERT를 학습했습니다. 이후 MRC도 Hugging Face를 활용할 계획이기 때문에, 해당 방법이 가장 좋다고 판단했습니다. BERT 학습은 해당 <a href="https://github.com/baseballChatbot7/KBOBERT/blob/main/KBOBERT_pre-training.py">repo</a>를 참고하시면 됩니다.
그리고 학습은 V100으로 이루어졌습니다.</p>

<h2 id="test">Test</h2>

<p>정상적으로 BERT가 학습되는지 확인하기 위해, KBO 뉴스 기사 약 46,000건으로 학습 후 테스트해 보았습니다.
그 <a href="https://github.com/baseballChatbot7/KBOBERT/wiki/Test-1">결과</a>, 야구 도메인에 관련된 단어를 [MASK] 처리하더라도 예측을 잘하는 모습을 확인할 수 있었습니다. 이로 미루어 보아, 학습 데이터를 더 추가한다면 충분히 사용할만한 BERT가 학습될 것 같습니다.</p>
:ET